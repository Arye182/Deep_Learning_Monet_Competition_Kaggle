\documentclass{article}
\usepackage[utf8]{inputenc}

\title{Deep Learning- Final Course Project}
\author{Arye Amsalem (ID. 123456789),Miri Yungreis ID. 987654321)}
\date{Submitted as final project report for the DL course, BIU, 2021}

\usepackage{natbib}
\usepackage{graphicx}
\usepackage{hyperref}
\usepackage{subcaption}
\usepackage[export]{adjustbox}
\usepackage{wrapfig}
\graphicspath{{images/}}
\begin{document}
\maketitle

\section{Introduction}
    The project is a part of a final assignment in course deep learning, as part of Kaggle competition Iâ€™m Something of a Painter Myself in which we can actually take photos from life and translate them into Monet styled photos. Adding style to photos is a very popular issue in programming nowadays, we see a lot of filters and image processing applications on a daily basis. The ability to take photos and mimic the style of an artist is a quite impressive challenge.
    \section{Solution}
    \subsection{General approach}
    as a general approach to solve the problem we will take the following steps, starting with:
    \\*
    \textbf{Steps 1,2:}
    take a baseline model of cycleGan and see results of full Monet data (300)
    take the same one and see results only with 30 pictures of Monet
    steps 1 and 2 are just to get the feeling of the results and behaviour of where we are and what is our destination.
    \\*
    Now, after these two steps we need to realize how to get better results.
    The first step will be - trying to enlarge the amount of pictures we have from 30 to something bigger - 30 train examples are way too little in deep learning especially in image processing and translating. reading on that issue led us to a main technique of doing that: image augmentation in which we can take our 30 monet images and duplicate them with changes like flipping, rotating, cutting, blurring, coloring etc. by that we increase our Monet data.
    \\*
    \textbf{Step 2.5} - choosing the right 30 photos out of 300
    Since we are dealing with images we want to make sure that we take the right 30 pictures out of the 300.
    we will scan the 300 photos with quality analyzer ()
    Then we will histogram the colors and take at least one of each color by finding the dominant color in each photo. We will take 30 such dominant colors that differ from each other to enlarge the variety of our data - the phase of chosing photos by dominant color unfortunately was not examined due to technical issues in the project.
    \\*
    \textbf{Step 3:}
    run the baseline model with 300 augmented monets.
    Here we will test all kinds of augmentation methods to see which are the most effective ones. will be
    3.1 - RandomFlip, RandomRotation
    3.2 - RandomFlip, RandomRotation, Crops
    after taking care of the data issue we can do more things to improve our results:
    \\*
    \textbf{Step 4} - further improvements on the best augmented trial
    check epochs results - how many epochs are needed to fit the model the best?
    learning rates of model - try different values
    \\*
    \textbf{Step 5 }- further ideas that we werent managed to obtain due to time constraints:
    (1) transfer learning - idea to take a pre-trained gan model for styling images or image to image model, and train this model with our data or other idea to take another data of image transfering and teach our model basic shapes first, and start our model training from a point that he already know objects in our world(maybe landscape imagnet dataset).
    (2) research other models then cycle gan
    (3) playing with the model base architecture - switching loss functions, adjusting residual blocks etc.
    unfortunately we could not managed to do this expeirments.
    \subsection{Design}
    Platform
    We write a python notebook in the format of an ipynb file. There are a couple of platforms giving the development environment. kaggle, jupyter, colab. we chose colab just to be with no limitations that kaggle gave and convenience.
    The running time of the training was a huge factor in choosing the configuration of the notebook, apparently there is a TPU engine provided in colab that is way faster than others by far.
    an epoch is :\\*
    cpu: 6500 seconds\\*
    GPU: 2500 seconds\\*
    TPU: 55 seconds\\*
    So obviously TPU was our favorite, in regard to the library we chse tensor flow-keras after understanding that it is more suitable for TPU.
    \textbf{Challenges}
    it was very challenging to handle the amount of experiments to succeed with obtaining high score.
    working with google colab, kaggle, google drive was quit challenging - the free versions are not good enough to run multiple tests with minimum time. most of the time we were struggling with handling syntax python issues in the notebook and integrating colab with kaggle etc.
    eventually we switched to colab pro with a much faster GPU.
    Speaking of The CycleGan Model We Took The Baseline model that was introduced in the tutorial notebook in kaggle [4] - which is also based on the basic model CycleGan that was introduced also in keras documentation as example of the horses-zebras and the original implementation of cycleGan. we decided to take it as a baseline model and uses its architecture (generators, discriminators, residuals, loss, optimizers)

\section{Experimental results}
after doing some trials these are the results\\*
    \begin{tabular}{ |p{1.5cm}|p{1.5cm}|p{1.5cm}| p{1.5cm}|p{1.5cm}|p{1.5cm}| }
    \hline
    \multicolumn{6}{|c|}{Experiments Results} \\
    \hline
    Trial & Model & Monet-Data-Size & FID & Kaggle Score & EPOCHS\\
    \hline
    1   & baseline & 300 & 13.72 & 1 & 25 \\
    2   & baseline & 30  & 17.27 & 1 & 25 \\
    1.1 & baseline & 300 & 13.17 & 1 & 48 \\
    2.1 & baseline & 30  & 14.54 & 1 & 48 \\
    3.1 & augmented & 300 & 12.71 & 1 & 30 \\
    3.2 & augmented & 30  & 1     & 1 & 47  \\
    \hline
    \end{tabular}
    %% --- 1 --- %%
    \begin{figure}
        \includegraphics[width=14cm, left]{image8.png}
        \caption{Trial 1 Loss Results}
    \end{figure}
    %% --- 2 --- %%
    \begin{figure}
        \includegraphics[width=14cm, left]{image3.png}
        \caption{Trial 2 Loss Results}
    \end{figure}
    %% --- 1.1 --- %%
    \begin{figure}
        \includegraphics[width=14cm, left]{image1.png}
        \caption{Trial 1.1 Loss Results}
    \end{figure}
    \begin{figure}
        \includegraphics[width=14cm, left]{image7.png}
        \caption{Trial 1.1 FID}
    \end{figure}
    %% --- 2.1 --- %%
    \begin{figure}
        \includegraphics[width=14cm, left]{image4.png}
        \caption{Trial 2.1 Loss Results}
    \end{figure}
    \begin{figure}
        \includegraphics[width=14cm, left]{image5.png}
        \caption{Trial 2.1 FID}
    \end{figure}
    %% --- 3.1 --- %%
    \begin{figure}
        \includegraphics[width=14cm, left]{image2.png}
        \caption{Trial 2.1 Loss Results}
    \end{figure}
    \begin{figure}
        \includegraphics[width=14cm, left]{image6.png}
        \caption{Trial 2.1 FID}
    \end{figure}
    %% --- 3.2 --- %%
    \\*
    results of trial 3.2:\\*
    After epoch #36 FID = [[12.74711]]\\*
    After epoch #37 FID = [[12.949053]]\\*
    After epoch #38 FID = [[13.508417]]\\*
    After epoch #39 FID = [[13.065748]]\\*
    After epoch #40 FID = [[12.842812]]\\*
    After epoch #41 FID = [[12.941837]]\\*
    After epoch #42 FID = [[13.384985]]\\*
\section{Discussion}
We learned two metrics for the model quality: FID and Loss, Although we used the lost function many time during the project, we eventually trusted the Fid.
the reason for that is the fact that regarding the Discriminator the lost does not always indicate with 100 percent certainty high loss does not mean that the model is not good and vice versa.
we learned how to work with data augmentation, how to take 30 pictures and make them become 300, but still be different from each other.

In addition to the 2 augmentation methods that we eventually used we also ran 3 kind of corp augmentation.

\section{Code}
this is the link to our github repository has the training notebook:
there you will have instructions and further testing and running details
\\*
\url{https://github.com/Arye182/Deep_Learning_Monet_Competition_Kaggle}
\\*
this is our inference notebook\\*
\url{https://www.kaggle.com/aryeamsalem/inference}
%%%%%%%%%%% bibliography %%%%%%%%%%%%%%%%%%%%%%%%%%
\bibliographystyle{plain}
\bibliography{references}
\begin{thebibliography}{100}
\bibitem{1 - CycleGan Article} https://arxiv.org/pdf/1703.10593.pdf
\bibitem{2 - CycleGan Documentation}https://www.tensorflow.org/tutorials/generative/cyclegan
\bibitem{3 - CycleGan Implementation} https://www.youtube.com/watch?v=4LktBHGCNfw
\bibitem{4 - tuturial notebook} https://www.kaggle.com/amyjang/monet-cyclegan-tutorial
\bibitem{5 - used for fid} https://www.kaggle.com/unfriendlyai/cyclegan-with-dg-pretraining
\bibitem{6 - used fo helping in loss graph}https://www.kaggle.com/matkneky/monet-cyclegan-trials
\bibitem{7 - augmentation}https://www.tensorflow.org/tutorials/images/

\end{thebibliography}
%%%%%%%%%% end bibliography %%%%%%%%%%%%%%%%%%%%%%%
\end{document}